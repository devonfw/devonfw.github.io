{"type":"solution","filename":"microservices_azure_aks/index.asciidoc","title":"Microservice Platforms Solutions - Azure Kubernetes Service","breadcrumbs":[],"text":"//Platform=Azure\n//Maturity level=Complete\n\n:toc: macro\ntoc::[]\n:idprefix:\n:idseparator: -\n\n== Microservice Platforms Solutions - Azure Kubernetes Service\n=== Infrastructure\n==== Overview\n\nThe solution is to use Azure Kubernetes Service and the following platform features regarding. The focus of this chapter is to introduce the relevant features. Recommendations for a concrete setup are given in the next chapter.\nThe platform features that (can) complement Kubernetes are:\n\nThe services that (can) complement Kubernetes:\n\n* Advisory\n+\n--\nProactive and actionable recommendations from Azure Advisor based on your configuration and usage telemetry as described here.\n--\n* Provisioning\n+\n--\nUse Bridge to Kubernetes to iteratively develop, test, and debug microservices targeted for AKS clusters. It is a https://azure.microsoft.com/de-de/updates/azure-dev-spaces-is-retiring-on-31-october-2023/[client-only experience] offered through extensions in Visual Studio and Visual Studio Code. See also Provisioning for general aspects and service options for creating pipelines for creating infrastructure.\n--\n* Compliance\n+\n--\nUse security measures on networking level to avoid public IPs. Combine AKS with additional services to control ingress and outgoing traffic such as Application Gateway or firewalls.\n\nEnforce compliance rules to your cluster and CI/CD pipeline consistently with Azure Policy. Azure Active Directory provides access control with role-based-access-controls (RBAC) and service principals/ managed identities to back RBAC roles. Integration with Azure Security Center can provide security management, intelligent threat detection and actionable recommendations. \n--\n* Desaster recovery\n+\n--\nHigher availability using redundancies across availability zones, protecting applications from datacenter failures. Paired region deployment for disaster recovery.\n--\n* Monitoring\n+\n--\nFor infrastructure monitoring see \"Monitoring\". For infrastructure monitoring specific pages in Azure Monitor exist which will be described here.\n--\n\nThe picture below summarizes some of the services mentioned above:\n\nimage::aks_overview.png[AKS Overview, width=794, height=568]\n\n==== Detailed Native Setup\n\nAKS comes with the following additional features regarding the *orchestration platform*: \n\n* Scaling\n+\n--\nVirtual nodes enable network communication between pods that run in Azure Container Instances (ACI) and the AKS cluster. To provide this communication, a virtual network subnet is created and delegated permissions are assigned. Virtual nodes only work with AKS clusters created using advanced networking (Azure CNI). By default, AKS clusters are created with basic networking (kubenet). This implementation allows fast scaling of services for unpredictable traffic.\n\nimage::virtual_nodes.png[alt=Virtual Nodes,width=1394, height=746]\n\n--\n* Compliance\n+\n--\n*Kubernet-dashboard* has been used in the past for attacks. Either disable it or mitigate the attack potential by adding token security.\n\nAKS used to rely on *service principals* (SP), which are technical principals in AAD. They came with some challenges like being stored on the nodes and the need to rotate them. SPs allowed an AKS cluster to interact with other Azure resources. Managed service identity (MSI) is a wrapper for SP, it provides auto rotation and if its system assigned then it has the same life cycle as the resource. AKS uses several managed identities for built-in services and add-ons as described https://docs.microsoft.com/en-us/azure/aks/use-managed-identity[here]:\n\n** Control plane\n** Agent pool\n** Pod identity (Preview as of 13.10.2021)\n\n*Role based access control* (RBAC) can be used for Authentication and Authorization. Predefined roles are: Cluster admin/ Cluster user = developer. Setting it up requires (1) setting up a cluster role and (2) binding the Cluster Role to a user/ cluster identity.\n\n*Operating system security patches* are automatically pushed to your nodes on a nightly schedule. AKS uses a security hardened operating system image which complies to CIS benchmark.\n\n*Network security* can be used to reduce attack potential. A private AKS is located inside a VNET and only exposes the ingress controller with a public IP. Interact with the Kubernetes API server as a private endpoint using Azure Private Link. To improve cluster security and minimize attacks, the API server should only be accessible from a limited set of IP address ranges.\n\n*Azure policies* offer built-in policies to secure pods and built-in initiatives which map to pod security policies. They replace pod security policies and work with gatekeeper/ Open Policy Agent (OPA) under the hood as described here. In Kubernetes, Admission Controllers enforce semantic validation of objects during create, update, and delete operations. With Open Policy Agent (OPA) you can enforce custom policies on Kubernetes objects without recompiling or reconfiguring the Kubernetes API server.\n\nThe principle of least privilege should be applied to how traffic can flow between pods in an Azure Kubernetes Service (AKS) cluster. Let's say you likely want to block traffic directly to back-end applications. The *Network Policy feature* in Kubernetes lets you define rules for ingress and egress traffic between pods in a cluster. https://docs.microsoft.com/en-us/azure/aks/use-network-policies[Two options] for implementation exist:\n\n** Azure's own implementation, called Azure Network Policies.\n** Calico Network Policies, an open-source network and network security solution founded by Tigera.\n--\n* Provisioning\n+\n--\nEnvironments can be created by logical isolation (namespaces) or phyiscal (multiple clusters). Logical is usually recommended for dev/test whereas pysical for prod.\n--\n\n==== Variations\n\nThe following additional extra tools can be used in conjunction with Kubernetes:\n\n* Extensions orchestration platform\n+\n--\nDAPR is a distributed framework that provides application services to applications such as publish subscribe as described here.\nOpen Service Mesh (OSM) add-on for Azure Kubernetes Service (AKS) is now available in public preview as of 10.09.2021. OSM implements Service Mesh Interface to provide the most common service mesh features as described here.\n--\n* Configuration\n+\n--\nTools for accessing sensitive information:\n\n** *FlexDriver:* Meet rigorous compliance requirements with secrets centrally stored outside of clusters. Access application-specific keys, secrets, and certificates natively within Kubernetes from Azure Key Vault. Mount Azure Key Vault stores via flexvolume driver https://github.com/Azure/kubernetes-keyvault-flexvol\n\n** *Azure Key Vault to Kubernetes (akv2k8s):* Used for our applications, to make Azure Key Vault secrets, certificates and keys available to use in a secure way. The goals of Azure Key Vault to Kubernetes are to avoid a direct program dependency for getting secrets, secure and low risk to transfer Azure Key Vault secrets and transparently inject secrets into applications. Per default secrets, configurations and certificates can be easily read and accessed by users in Kubernetes and access to them can only be restricted by setting access rights. This will be avoided and is a huge benefit of using akv2k8s with a simple setup of Azure Key Vault and the option to set more detailed restrictions and configurations.\n\n** https://github.com/Azure/secrets-store-csi-driver-provider-azure/[secrets-store-csi-driver] better rated than (akv2k8s). If you want to get from multiple keyvaults secrets using multiple identities you had to have multiple instances of akv2k8s running. And secret store csi driver is more mature in general.\n\nTools for synchronizing configuration:\n\n** *ArgoCD:* ArgoCD is a leader developed by Intuit and synchronizes changes in the code of applications, photos and cluster definitions, so the Git - repository, to the cluster. The solution is open source software, kept relatively simple and is one of the most important and oldest tools on the market.\n** *Flux:* Flux basically does the same job as ArgoCD, i.e. synchronizes the repository and cluster in the course of continuous delivery. Flux is also open source and kept simple - the special thing: It comes from the GitOps inventor Weaveworks.\n--\n\n==== When to use\n\nWhen you need a container platform with orchestration support due to a higher number of microservices in Azure. Additional built in functionality that comes with Azure Red Hat OpenShift is not required or does not fit into your requirements (e.g. because you want to use a different tooling or need more flexibility.)\n\n=== Application\n==== Overview\n\nThe solution is to deploy the containerized application to an Azure Kubernetes Service. Focus of that chapter are designing, building, monitoring and deploying containerized applications. Recommendations for a concrete setup are given in the next chapter.\n\nThe services that (can) complement Kubernetes:\n\n* Designing\n+\n--\nEach Pod is meant to run a single instance of a given application. If you want to scale your application horizontally (to provide more overall resources by running more instances), you should use multiple Pods, one for each instance. In Kubernetes, this is typically referred to as replication. Replicated Pods are usually created and managed as a group by a workload resource and its controller.\n\nThe \"one-container-per-Pod\" model is the most common Kubernetes use case. A more advanced use case is running multiple containers in a pod that need to work together. A Pod can encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers form a single cohesive unit of service - for example, one container serving data stored in a shared volume to the public, while a separate sidecar container refreshes or updates those files. The Pod wraps these containers, storage resources, and an ephemeral network identity together as a single unit.\n\nContainers have to store information persistently. https://docs.microsoft.com/en-us/azure/aks/concepts-storage[Azure] provides Azure (managed) disks and Azure files as storage options for persistent volumes.\nAKS can connect with databases via wrapper objects such as Services  or databases can be directly deployed to Kubernetes. Options for deplyong a database directly to Kubernetes are given below:\n\n** SQL server (Microsoft): Options range from single https://docs.microsoft.com/en-us/sql/linux/tutorial-sql-server-containers-kubernetes?view=sql-server-ver15[sql server] to https://docs.microsoft.com/en-us/sql/linux/tutorial-sql-server-containers-kubernetes-dh2i?view=sql-server-ver15[high availability] with failover groups. In both cases MS provides containers that contain sql server.\n** Third party options such as https://portworx.com/blog/ha-postgresql-azure-aks/[PostgreSQL]\n--\n* Configuration\n+\n--\nConfigmaps are useful to store non-critical data in key-value pair format. They can also be used to inject env vars into pods. Secrets are useful to store sensitive data in key value pair format. They can also be used to inject env vars into pods. You can optionally specify how much of each resource a container needs. The most common resources to specify are CPU and memory (RAM).\n--\n* Compliance\n+\n--\nA https://kubernetes.io/docs/tasks/configure-pod-container/security-context/[security context] defines privilege and access control settings for a pod. Examples are: \n\n** Discretionary Access Control: Permission to access an object, like a file, is based on user ID (UID) and group ID (GID).\n** Security Enhanced Linux (SELinux): Objects are assigned security labels.\n** Running as privileged or unprivileged.\n** Linux Capabilities: Give a process some privileges, but not all the privileges of the root user.\n** AppArmor: Use program profiles to restrict the capabilities of individual programs.\n** AllowPrivilegeEscalation: Controls whether a process can gain more privileges than its parent process.\n** readOnlyRootFilesystem: Mounts the container's root filesystem as read-only.\n\nDisks used in your AKS cluster can by encrypted by using your own keys through Azure Key Vault.\n\nSee building for additional security measures when containers are built.\n--\n* Building (CI part of provisioning)\n+\n--\nBuilding containers includes the following steps:\n\n1. Building the container image(s)\n2. Pushing the image(s) to the registry\n\nProvisioning tools such as Azure DevOps and Gitub Actions provide special docker tasks/ activities to build images. Pushing to registries is also supported. The following additional features can be used/ should be considered from security perspective:\n\n** Each time a *base image is updated*, you should also update any downstream container images. Integrate this build process into validation and deployment pipelines such as Azure Pipelines or Jenkins. These pipelines make sure that your applications continue to run on the updated based images. Once your application container images are validated, the AKS deployments can then be updated to run the latest, secure images. Azure Container Registry Tasks can also https://docs.microsoft.com/en-us/azure/aks/operator-best-practices-container-image-management[automatically update container images] when the base image is updated.\n**  https://docs.microsoft.com/en-us/azure/aks/operator-best-practices-container-image-management[A container security scan] can be included in the *pipelines as quality gate* by using tools like tools such as Twistlock or Aqua.\n** The https://docs.microsoft.com/en-us/azure/devops/pipelines/ecosystems/containers/content-trust?view=azure-devops[provisioning services support] Docker Content Trust (DCT). Docker Content Trust (DCT) are digital signatures for data sent to and received from remote Docker registries. These signatures allow client-side or runtime verification of the integrity and publisher of specific image tags.\n--\n* Deployment (CI part of provisioning)\n+\n--\nThe term \"Deployment\" refers to the process that triggers a deployment in Kubernetes whereas a Kubernetes deployment refers to the Kubernets deployment resource. A kubernetes deployment resource is the standard controller for manipulating pods which in turn host the container workloads.\n\nA deployment is triggered by the provisioning pipeline.  Depending on the scope a deployment goes beyond the a kubernetes deployment that results in a Kubernetes deployment resource. The various steps across various scenarios can be generalized as follows:\n\n1. Pre-Kubernetes Deployment steps\n2. Kubernetes Deployment\nAzure provisioning services provide ways to trigger with native kubernetes means such as manifests by supporting special tasks/ activties. However, this results in quite a number of files you have to maintain. Additional tools like https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/helm-deploy?view=azure-devops[helm] (see variation) provide better support.\n3. Post-Kubernetes Deployment steps\n\nThe most complex deployment scenario is a rolling update with breaking database changes. In that case pre and post Kubernetes deployment steps are required to https://stackoverflow.com/questions/48877182/kubernetes-rolling-deployments-and-database-migrations/48880687[handle the breaking database changes]. Such an update requires targeting specific components e.g. with a certain version. Labels are key/value pairs that are attached to objects, such as pods. They help in filtering out specific objects. Using a Selector, the client/user can identify a set of objects. Annotations are used to attach arbitrary non-identifying metadata to objects.\n\nThe basic idea is to break down the breaking database change into multiple non-breaking steps. The https://stackoverflow.com/questions/48877182/kubernetes-rolling-deployments-and-database-migrations/48880687[steps below] refer to a renaming of a column:\n\n1. Add a db migration that inserts the new column\n2. Change the app so that all writes go to the old and new column\n3. Run a task that copies all values from the old to the new column\n4. Change the app that it reads from the new column\n5. Add a migration that remove the old column\n--\n* Monitoring\n+\n--\nApplication logs can help in understanding the activities and status of the application. The logs are particularly useful for debugging problems and monitoring cluster activity. Monitoring applications can be done by storing logs and studying the applicationâ€™s metrics.\n\nTools like Prometheus-Grafana are popular as they make the management of metrics very easy. Very often, sidecar containers are used as metrics exporters of the main application container.\n\nBy https://docs.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-prometheus-integration[integrating with Azure Monitor], a Prometheus server is not required. You just need to expose the Prometheus metrics endpoint through your exporters or pods (application), and the containerized agent for Container insights can scrape the metrics for you.\n\nimage::mon_cnt_insights.png[alt=Monitoring Container Insights,width=1706, height=632]\n--\n\n==== Variations\n\nThe following additional extra tools can be used in conjunction with Kubernetes:\n\n* Deployment\n+\n--\nInstead of having to write separate YAML files for each application manually, you can simply create a Helm chart and let Helm deploy the application to the cluster for you. Helm charts contain templates for various Kubernetes resources that combine to form an application. \n\nimage::helm_chart_example.png[alt=Helm chart example]\n\nA Helm chart can be customized when deploying it on different Kubernetes clusters. Helm charts can be created in such a way that environment or deployment-specific configurations can be extracted out to a separate file so that these values can be specified when the Helm chart is deployed. The snippet below shows a template using placeholders to refer to the values in values.yaml:\n```YAML\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Values.postgres.name }}\n  labels:\n    app: {{ .Values.postgres.name }}\n    group: {{ .Values.postgres.group }}\nspec:\n  replicas: {{ .Values.replicaCount }}\n  selector:\n    matchLabels:\n      app: {{ .Values.postgres.name }}\n      ...\n```\n\n--\n* Compliance\n+\n--\nFor security reasons and improvement of Helm charts, it is useful to make use of at least one Helm linting tool to ensure your deployments are valid and versioned correctly.\n\nWhy choosing Polaris as Linting Tool: For helm chart linting, there are several tools like Polaris, kube-score or config-lint available. With Polaris, checks and rules are already given by default, whereby other tools need a lot of custom rules configuration and are therefore more complex to setup. Polaris runs a variety of checks to ensure that Kubernetes pods and controllers are configured using best practices, helping to avoid problems in the future. Polaris can be either installed inside a cluster or as a command-line tool to analyze Kubernetes manifests statically.\n--\n* Configuration\n+\n--\nSee under infrastructure.\n--\n\n==== When to use\n\nWhen you want to deploy containerized applications to Azure Kubernetes Service.\n\n== Credits\n\nimage::ms_guild_logo.png[MS Guild Logo, width=160, height=75, align=right, link=\"https://forms.office.com/Pages/ResponsePage.aspx?id=Wq6idgCfa0-V7V0z13xNYal7m2EdcFdNsyBBMUiro4NUNllHQTlPNU9QV1JRRjk3TTAwVUJCNThTRSQlQCN0PWcu\"]\n"}